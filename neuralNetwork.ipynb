{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "\n",
    "import autograd.numpy as np\n",
    "from autograd import grad, elementwise_grad\n",
    "import autograd.numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.size'] = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define ODE as:\n",
    "\n",
    "$\\dfrac{du}{dt}=\\mathrm{sin}(40\\,t\\,u)$ s.t. $u(0)=u_0=1$.  **(1)**\n",
    "\n",
    "That means a trial solution could be\n",
    "\n",
    "$u_t(t,P)=h_1(t)+h_2(t, N(t,P))$  **(2)** with $N(x,P)$ as the network of weights/biases $P$. We know $u(0)=u_0$, so we can write the trial solution as:\n",
    "\n",
    "$u_t(t,P)=u_0+tN(t,P)$  **(3)** which ensured that when $t=0$, $u_t(0)=u_0 + 0$ as desired.\n",
    "\n",
    "Network cost function will be squared residuals $c(x,P)=\\left[u_t'(t,P)-\\mathrm{sin}(40\\,t\\,u_t(x,P))\\right]^2$  **(4)** with associated minimisation problem $\\mathrm{min}_P\\:\\left[c(x,P)\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common to use some form of automatic differentiation to process the backpropagation steps of network training. Popular library autograd has been superseded by 'jax'. Can't install jax on windows at the moment though, so stick with autograd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equation (3)\n",
    "def trialSolution(t, P, u0=1):\n",
    "    return u0*(t*neural_network(P, t))\n",
    "\n",
    "# RHS of equation (1)\n",
    "def du(t, trialSolution):\n",
    "    return np.sin(40 * t * trialSolution)\n",
    "\n",
    "# Activation function for the network\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Gradient of the activation function\n",
    "def sigmoid_grad(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming one input, hidden, and output layer\n",
    "def neural_network(params, t):\n",
    "\n",
    "    # Find the weights (including and biases) for the hidden and output layer.\n",
    "    # Assume that params is a list of parameters for each layer.\n",
    "    # The biases are the first element for each array in params,\n",
    "    # and the weights are the remaning elements in each array in params.\n",
    "\n",
    "    w_hidden_1 = params[0]\n",
    "    w_hidden_2 = params[1]\n",
    "    w_output = params[2]\n",
    "\n",
    "    #t = np.asarray(t)\n",
    "    # Assumes input x being an one-dimensional array\n",
    "    num_values = np.size(t)\n",
    "    t = t.reshape(-1, num_values)\n",
    "    \n",
    "    # Assume that the input layer does nothing to the input t\n",
    "    t_input = t\n",
    "\n",
    "    ## Hidden layer 1:\n",
    "\n",
    "    # Add a row of ones to include bias\n",
    "    t_input = np.concatenate((np.ones((1, num_values)), t_input ), axis = 0)\n",
    "\n",
    "    z_hidden_1 = np.matmul(w_hidden_1, t_input)\n",
    "    t_hidden_1 = sigmoid(z_hidden_1)\n",
    "\n",
    "    ## Hidden layer 2:\n",
    "\n",
    "    # Add a row of ones to include bias\n",
    "    t_input_1 = np.concatenate((np.ones((1, num_values)), t_hidden_1 ), axis = 0)\n",
    "\n",
    "    z_hidden_2 = np.matmul(w_hidden_2, t_input_1)\n",
    "    t_hidden_2 = sigmoid(z_hidden_2)\n",
    "    \n",
    "    ## Output layer:\n",
    "\n",
    "    # Include bias:\n",
    "    t_hidden_2 = np.concatenate((np.ones((1,num_values)), t_hidden_2 ), axis = 0)\n",
    "\n",
    "    z_output = np.matmul(w_output, t_hidden_2)\n",
    "    t_output = z_output\n",
    "\n",
    "    return t_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost function\n",
    "def cost_function(P, t):\n",
    "\n",
    "    # Evaluate the trial function with the current parameters P\n",
    "    u_t = trialSolution(t, P)\n",
    "\n",
    "    # Find the derivative w.r.t t of the neural network\n",
    "    d_net_out = elementwise_grad(neural_network, 1)(P, t)\n",
    "\n",
    "    # Find the derivative w.r.t t of the trial function\n",
    "    d_g_t = elementwise_grad(trialSolution, 0)(t, P)\n",
    "    \n",
    "    # Error terms\n",
    "    err_sqr = (d_g_t - du(t, u_t))**2\n",
    "    cost_sum = np.sum(err_sqr)\n",
    "\n",
    "    return cost_sum / np.size(err_sqr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the exponential decay ODE using neural network with one input, hidden, and output layer\n",
    "def solve_ode_neural_network(t, num_neurons_hidden, num_iter, lmb):\n",
    "    ## Set up initial weights and biases\n",
    "\n",
    "    # For the 1st hidden layer\n",
    "    p0 = npr.randn(num_neurons_hidden, 2)\n",
    "\n",
    "    # For the 2st hidden layer\n",
    "    p1 = npr.randn(num_neurons_hidden, 2)\n",
    "    \n",
    "    # For the output layer\n",
    "    p2 = npr.randn(1, num_neurons_hidden + 1) # +1 since bias is included\n",
    "\n",
    "    P = [p0, p1, p2]\n",
    "\n",
    "    print('Initial cost: %g'%cost_function(P, t))\n",
    "\n",
    "    ## Start finding the optimal weights using gradient descent\n",
    "\n",
    "    # Find the Python function that represents the gradient of the cost function\n",
    "    # w.r.t the 0-th input argument -- that is the weights and biases in the hidden and output layer\n",
    "    cost_function_grad = grad(cost_function, 0)  # arg1 is the actual function defined above.\n",
    "\n",
    "    # Let the update be done num_iter times\n",
    "    for i in range(num_iter):\n",
    "        # Evaluate the gradient at the current weights and biases in P.\n",
    "        # The cost_grad consist now of two arrays;\n",
    "        # one for the gradient w.r.t P_hidden and\n",
    "        # one for the gradient w.r.t P_output\n",
    "        cost_grad =  cost_function_grad(P, t)\n",
    "\n",
    "        P[0] = P[0] - lmb * cost_grad[0]\n",
    "        P[1] = P[1] - lmb * cost_grad[1]\n",
    "        P[2] = P[2] - lmb * cost_grad[2]\n",
    "        \n",
    "    print('Final cost: %g'%cost_function(P, t))\n",
    "\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "\n",
    "#K = 0.01  # Step size\n",
    "#tmin, tmax = 0, 1  # Domain\n",
    "#N = int(tmax/K)  # Number points in time to compute\n",
    "num_hidden_neurons = 10\n",
    "num_iter = 10000\n",
    "t = np.linspace(0, 1, 100) # Grid limits\n",
    "W = [npr.randn(1,10), npr.randn(10,1)] # Randomly initialise the weights - this is where it's decided that there'll be 10 hidden nodes?\n",
    "lamda = 0.001 # Gradient descent learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 11 is different from 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-784aaa9b4b24>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Do the network training and find the best parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msolve_ode_neural_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_hidden_neurons\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlamda\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# x is a vector here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Print the deviation from the trial solution and true solution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-22ff9ec60e28>\u001b[0m in \u001b[0;36msolve_ode_neural_network\u001b[1;34m(t, num_neurons_hidden, num_iter, lmb)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mp0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Initial cost: %g'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mcost_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m## Start finding the optimal weights using gradient descent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-6670bb4bd368>\u001b[0m in \u001b[0;36mcost_function\u001b[1;34m(P, t)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# Evaluate the trial function with the current parameters P\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mu_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrialSolution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# Find the derivative w.r.t t of the neural network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-cde828f446eb>\u001b[0m in \u001b[0;36mtrialSolution\u001b[1;34m(t, P, u0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Equation (3)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrialSolution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mu0\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mneural_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# RHS of equation (1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-78c33b420095>\u001b[0m in \u001b[0;36mneural_network\u001b[1;34m(params, t)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mt_input_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_hidden_1\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mz_hidden_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_hidden_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_input_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[0mt_hidden_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_hidden_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\autograd\\tracer.py\u001b[0m in \u001b[0;36mf_wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mnew_box\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf_raw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m     \u001b[0mf_wrapped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfun\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf_raw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0mf_wrapped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_autograd_primitive\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 11 is different from 2)"
     ]
    }
   ],
   "source": [
    "# Do the network training and find the best parameters\n",
    "\n",
    "P = solve_ode_neural_network(t, num_hidden_neurons, num_iter, lamda)  # x is a vector here\n",
    "\n",
    "# Print the deviation from the trial solution and true solution\n",
    "result = trialSolution(t, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,4))\n",
    "ax.plot(t, result.T, linewidth=3, label='Neural network result')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
